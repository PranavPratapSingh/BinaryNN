{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:45: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from . import h5a, h5d, h5ds, h5f, h5fd, h5g, h5r, h5s, h5t, h5p, h5z\n",
      "/usr/local/lib/python2.7/dist-packages/h5py/_hl/group.py:22: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from .. import h5g, h5i, h5o, h5r, h5t, h5l, h5p\n",
      "Using TensorFlow backend.\n",
      "/home/pps/.local/lib/python2.7/site-packages/requests/__init__.py:83: RequestsDependencyWarning: Old version of cryptography ([1, 2, 3]) may cause slowdown.\n",
      "  warnings.warn(warning, RequestsDependencyWarning)\n",
      "/usr/local/lib/python2.7/dist-packages/scipy/sparse/lil.py:19: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from . import _csparsetools\n",
      "/usr/local/lib/python2.7/dist-packages/scipy/sparse/csgraph/__init__.py:165: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._shortest_path import shortest_path, floyd_warshall, dijkstra,\\\n",
      "/usr/local/lib/python2.7/dist-packages/scipy/sparse/csgraph/_validation.py:5: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._tools import csgraph_to_dense, csgraph_from_dense,\\\n",
      "/usr/local/lib/python2.7/dist-packages/scipy/sparse/csgraph/__init__.py:167: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._traversal import breadth_first_order, depth_first_order, \\\n",
      "/usr/local/lib/python2.7/dist-packages/scipy/sparse/csgraph/__init__.py:169: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._min_spanning_tree import minimum_spanning_tree\n",
      "/usr/local/lib/python2.7/dist-packages/scipy/sparse/csgraph/__init__.py:170: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._reordering import reverse_cuthill_mckee, maximum_bipartite_matching, \\\n",
      "/usr/local/lib/python2.7/dist-packages/scipy/linalg/basic.py:17: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._solve_toeplitz import levinson\n",
      "/usr/local/lib/python2.7/dist-packages/scipy/linalg/__init__.py:207: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._decomp_update import *\n",
      "/usr/local/lib/python2.7/dist-packages/scipy/special/__init__.py:640: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._ufuncs import *\n",
      "/usr/local/lib/python2.7/dist-packages/scipy/special/_ellip_harm.py:7: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._ellip_harm_2 import _ellipsoid, _ellipsoid_norm\n",
      "/usr/local/lib/python2.7/dist-packages/scipy/interpolate/_bsplines.py:10: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from . import _bspl\n",
      "/usr/local/lib/python2.7/dist-packages/scipy/spatial/__init__.py:95: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from .ckdtree import *\n",
      "/usr/local/lib/python2.7/dist-packages/scipy/spatial/__init__.py:96: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from .qhull import *\n",
      "/usr/local/lib/python2.7/dist-packages/scipy/spatial/_spherical_voronoi.py:18: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from . import _voronoi\n",
      "/usr/local/lib/python2.7/dist-packages/scipy/spatial/distance.py:122: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from . import _hausdorff\n",
      "/usr/local/lib/python2.7/dist-packages/scipy/ndimage/measurements.py:36: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from . import _ni_label\n",
      "/home/pps/.local/lib/python2.7/site-packages/pandas/_libs/__init__.py:4: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from .tslib import iNaT, NaT, Timestamp, Timedelta, OutOfBoundsDatetime\n",
      "/home/pps/.local/lib/python2.7/site-packages/pandas/__init__.py:26: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from pandas._libs import (hashtable as _hashtable,\n",
      "/home/pps/.local/lib/python2.7/site-packages/pandas/core/dtypes/common.py:6: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from pandas._libs import algos, lib\n",
      "/home/pps/.local/lib/python2.7/site-packages/pandas/core/util/hashing.py:7: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from pandas._libs import hashing, tslib\n",
      "/home/pps/.local/lib/python2.7/site-packages/pandas/core/indexes/base.py:7: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from pandas._libs import (lib, index as libindex, tslib as libts,\n",
      "/home/pps/.local/lib/python2.7/site-packages/pandas/tseries/offsets.py:21: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  import pandas._libs.tslibs.offsets as liboffsets\n",
      "/home/pps/.local/lib/python2.7/site-packages/pandas/core/ops.py:16: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from pandas._libs import algos as libalgos, ops as libops\n",
      "/home/pps/.local/lib/python2.7/site-packages/pandas/core/indexes/interval.py:32: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from pandas._libs.interval import (\n",
      "/home/pps/.local/lib/python2.7/site-packages/pandas/core/internals.py:14: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from pandas._libs import internals as libinternals\n",
      "/home/pps/.local/lib/python2.7/site-packages/pandas/core/sparse/array.py:33: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  import pandas._libs.sparse as splib\n",
      "/home/pps/.local/lib/python2.7/site-packages/pandas/core/window.py:36: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  import pandas._libs.window as _window\n",
      "/home/pps/.local/lib/python2.7/site-packages/pandas/core/groupby/groupby.py:68: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from pandas._libs import (lib, reduction,\n",
      "/home/pps/.local/lib/python2.7/site-packages/pandas/core/reshape/reshape.py:30: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from pandas._libs import algos as _algos, reshape as _reshape\n",
      "/home/pps/.local/lib/python2.7/site-packages/pandas/io/parsers.py:45: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  import pandas._libs.parsers as parsers\n",
      "/home/pps/.local/lib/python2.7/site-packages/pandas/io/pytables.py:50: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from pandas._libs import algos, lib, writers as libwriters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.backend import clear_session\n",
    "import keras\n",
    "from keras.models import clone_model,Sequential,save_model\n",
    "from keras.layers import Conv2D,Activation,MaxPool2D,Flatten,Dense,Dropout\n",
    "from keras.optimizers import nadam,rmsprop\n",
    "from keras.utils import to_categorical\n",
    "from keras.datasets import fashion_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def splitter(n, array):\n",
    "    arr = np.zeros(array.shape)\n",
    "    for ix in range(array.shape[0]):\n",
    "        if array[ix] >= n:\n",
    "            arr[ix] = 1\n",
    "        else:\n",
    "            arr[ix] = 0\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train_first = y_train\n",
    "y_test_first = y_test\n",
    "target = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_first = to_categorical(splitter(5,y_train))\n",
    "y_test_first = to_categorical(splitter(5,y_test))\n",
    "x_train = x_train/255.0\n",
    "x_test = x_test/255.0\n",
    "x_train = x_train.reshape(-1,28,28,1)\n",
    "x_test = x_test.reshape(-1,28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 26, 26, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 5408)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 5408)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 10818     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 11,138\n",
      "Trainable params: 11,138\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=Sequential()\n",
    "\n",
    "model.add(Conv2D(32,(3,3),input_shape=(28,28,1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPool2D(2,2))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='categorical_crossentropy' , optimizer=rmsprop(lr=0.001), metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 5s 91us/step - loss: 0.2341 - acc: 0.9010 - val_loss: 0.2011 - val_acc: 0.9180\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 0.1888 - acc: 0.9206 - val_loss: 0.1852 - val_acc: 0.9217\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 0.1737 - acc: 0.9265 - val_loss: 0.1854 - val_acc: 0.9236\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 0.1632 - acc: 0.9298 - val_loss: 0.1698 - val_acc: 0.9294\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 0.1558 - acc: 0.9336 - val_loss: 0.1575 - val_acc: 0.9331\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 0.1517 - acc: 0.9342 - val_loss: 0.1572 - val_acc: 0.9333\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 0.1484 - acc: 0.9359 - val_loss: 0.1525 - val_acc: 0.9354\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 0.1449 - acc: 0.9366 - val_loss: 0.1485 - val_acc: 0.9359\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 0.1427 - acc: 0.9368 - val_loss: 0.1465 - val_acc: 0.9366\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.1410 - acc: 0.9376 - val_loss: 0.1457 - val_acc: 0.9361\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(x_train,y_train_first,epochs=10,shuffle=True,batch_size=100,validation_data=(x_test,y_test_first))\n",
    "save_model(model,\"../Models/initial_model\",overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(arr,n):\n",
    "    target0 = []\n",
    "    target1 = []\n",
    "    for ix in arr :\n",
    "        if ix >= n:\n",
    "            target1.append(ix)\n",
    "        else:\n",
    "            target0.append(ix)\n",
    "    target0 = np.array(target0)\n",
    "    target1 = np.array(target1)\n",
    "    return target0,target1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_split(data_arr,target_arr,n):\n",
    "    data0 = []\n",
    "    data1 = []\n",
    "    for ix in range(target_arr.shape[0]) :\n",
    "        if target_arr[ix] >= n:\n",
    "            data1.append(data_arr[ix])\n",
    "        else:\n",
    "            data0.append(data_arr[ix])\n",
    "    data0 = np.array(data0)\n",
    "    data1 = np.array(data1)\n",
    "    return data0,data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_second_first, train_second_second = split(y_train,5)\n",
    "y_train_second_first = to_categorical(splitter(2,train_second_first))\n",
    "y_train_second_second = to_categorical(splitter(8,train_second_second))\n",
    "\n",
    "x_train_second_first,x_train_second_second = data_split(x_train,y_train,5)\n",
    "\n",
    "test_second_first, test_second_second = split(y_test,5)\n",
    "y_test_second_first = to_categorical(splitter(2,test_second_first))\n",
    "y_test_second_second = to_categorical(splitter(8,test_second_second))\n",
    "\n",
    "x_test_second_first,x_test_second_second = data_split(x_test,y_test,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 30000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "30000/30000 [==============================] - 2s 73us/step - loss: 0.1858 - acc: 0.9284 - val_loss: 0.1574 - val_acc: 0.9364\n",
      "Epoch 2/10\n",
      "30000/30000 [==============================] - 2s 67us/step - loss: 0.1291 - acc: 0.9524 - val_loss: 0.1253 - val_acc: 0.9544\n",
      "Epoch 3/10\n",
      "30000/30000 [==============================] - 2s 69us/step - loss: 0.1189 - acc: 0.9572 - val_loss: 0.1170 - val_acc: 0.9566\n",
      "Epoch 4/10\n",
      "30000/30000 [==============================] - 2s 66us/step - loss: 0.1108 - acc: 0.9596 - val_loss: 0.1139 - val_acc: 0.9596\n",
      "Epoch 5/10\n",
      "30000/30000 [==============================] - 2s 70us/step - loss: 0.1057 - acc: 0.9616 - val_loss: 0.1090 - val_acc: 0.9618\n",
      "Epoch 6/10\n",
      "30000/30000 [==============================] - 2s 66us/step - loss: 0.1027 - acc: 0.9637 - val_loss: 0.1051 - val_acc: 0.9630\n",
      "Epoch 7/10\n",
      "30000/30000 [==============================] - 2s 67us/step - loss: 0.0989 - acc: 0.9646 - val_loss: 0.1031 - val_acc: 0.9636\n",
      "Epoch 8/10\n",
      "30000/30000 [==============================] - 2s 68us/step - loss: 0.0952 - acc: 0.9656 - val_loss: 0.0996 - val_acc: 0.9646\n",
      "Epoch 9/10\n",
      "30000/30000 [==============================] - 2s 73us/step - loss: 0.0912 - acc: 0.9676 - val_loss: 0.1016 - val_acc: 0.9634\n",
      "Epoch 10/10\n",
      "30000/30000 [==============================] - 2s 71us/step - loss: 0.0896 - acc: 0.9678 - val_loss: 0.0964 - val_acc: 0.9678\n"
     ]
    }
   ],
   "source": [
    "model_second_first = clone_model(model)\n",
    "model_second_first.compile(loss='categorical_crossentropy' , optimizer=rmsprop(lr=0.001), metrics=['acc'])\n",
    "hist = model_second_first.fit(x_train_second_first,y_train_second_first,\n",
    "                 epochs=10,\n",
    "                 shuffle=True,\n",
    "                 batch_size=100,\n",
    "                 validation_data=(x_test_second_first,y_test_second_first))\n",
    "save_model(model_second_first,\"../Models/model_second_first\",overwrite=True)\n",
    "clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 30000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "30000/30000 [==============================] - 2s 73us/step - loss: 0.1956 - acc: 0.9293 - val_loss: 0.1372 - val_acc: 0.9498\n",
      "Epoch 2/10\n",
      "30000/30000 [==============================] - 2s 67us/step - loss: 0.1320 - acc: 0.9539 - val_loss: 0.1141 - val_acc: 0.9572\n",
      "Epoch 3/10\n",
      "30000/30000 [==============================] - 2s 67us/step - loss: 0.1124 - acc: 0.9610 - val_loss: 0.1032 - val_acc: 0.9606\n",
      "Epoch 4/10\n",
      "30000/30000 [==============================] - 2s 67us/step - loss: 0.1011 - acc: 0.9653 - val_loss: 0.0958 - val_acc: 0.9648\n",
      "Epoch 5/10\n",
      "30000/30000 [==============================] - 2s 67us/step - loss: 0.0936 - acc: 0.9688 - val_loss: 0.0923 - val_acc: 0.9650\n",
      "Epoch 6/10\n",
      "30000/30000 [==============================] - 2s 67us/step - loss: 0.0875 - acc: 0.9707 - val_loss: 0.0886 - val_acc: 0.9666\n",
      "Epoch 7/10\n",
      "30000/30000 [==============================] - 2s 69us/step - loss: 0.0837 - acc: 0.9725 - val_loss: 0.0880 - val_acc: 0.9680\n",
      "Epoch 8/10\n",
      "30000/30000 [==============================] - 2s 66us/step - loss: 0.0796 - acc: 0.9730 - val_loss: 0.0804 - val_acc: 0.9724\n",
      "Epoch 9/10\n",
      "30000/30000 [==============================] - 2s 67us/step - loss: 0.0777 - acc: 0.9740 - val_loss: 0.0787 - val_acc: 0.9716\n",
      "Epoch 10/10\n",
      "30000/30000 [==============================] - 2s 66us/step - loss: 0.0749 - acc: 0.9749 - val_loss: 0.0783 - val_acc: 0.9722\n"
     ]
    }
   ],
   "source": [
    "model_second_second = clone_model(model)\n",
    "model_second_second.compile(loss='categorical_crossentropy' , optimizer=rmsprop(lr=0.001), metrics=['acc'])\n",
    "hist = model_second_second.fit(x_train_second_second,y_train_second_second,\n",
    "                 epochs=10,\n",
    "                 shuffle=True,\n",
    "                 batch_size=100,\n",
    "                 validation_data=(x_test_second_second,y_test_second_second))\n",
    "save_model(model_second_second,\"../Models/model_second_second\",overwrite=True)\n",
    "clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_third_first, train_third_second = split(train_second_first,2)\n",
    "y_train_third_first = to_categorical(splitter(1,train_third_first))\n",
    "y_train_third_second = to_categorical(splitter(4,train_third_second))\n",
    "\n",
    "x_train_third_first,x_train_third_second = data_split(x_train_second_first,train_second_first,2)\n",
    "\n",
    "train_third_third, train_third_fourth = split(train_second_second,8)\n",
    "y_train_third_third = to_categorical(splitter(6,train_third_third))\n",
    "y_train_third_fourth = to_categorical(splitter(9,train_third_third))\n",
    "\n",
    "x_train_third_third,x_train_third_fourth = data_split(x_train_second_second,train_second_second,8)\n",
    "\n",
    "test_third_first, test_third_second = split(test_second_first,2)\n",
    "y_test_third_first = to_categorical(splitter(1,test_third_first))\n",
    "y_test_third_second = to_categorical(splitter(4,test_third_second))\n",
    "\n",
    "x_test_third_first,x_test_third_second = data_split(x_test_second_first,test_second_first,2)\n",
    "\n",
    "test_third_third, test_third_fourth = split(test_second_second,8)\n",
    "y_test_third_third = to_categorical(splitter(6,test_third_third))\n",
    "y_test_third_fourth = to_categorical(splitter(9,test_third_third))\n",
    "\n",
    "x_test_third_third,x_test_third_fourth = data_split(x_test_second_second,test_second_second,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "12000/12000 [==============================] - 1s 81us/step - loss: 0.1073 - acc: 0.9689 - val_loss: 0.0368 - val_acc: 0.9885\n",
      "Epoch 2/10\n",
      "12000/12000 [==============================] - 1s 67us/step - loss: 0.0349 - acc: 0.9876 - val_loss: 0.0266 - val_acc: 0.9895\n",
      "Epoch 3/10\n",
      "12000/12000 [==============================] - 1s 69us/step - loss: 0.0280 - acc: 0.9903 - val_loss: 0.0224 - val_acc: 0.9915\n",
      "Epoch 4/10\n",
      "12000/12000 [==============================] - 1s 68us/step - loss: 0.0253 - acc: 0.9913 - val_loss: 0.0226 - val_acc: 0.9895\n",
      "Epoch 5/10\n",
      "12000/12000 [==============================] - 1s 66us/step - loss: 0.0235 - acc: 0.9930 - val_loss: 0.0206 - val_acc: 0.9905\n",
      "Epoch 6/10\n",
      "12000/12000 [==============================] - 1s 66us/step - loss: 0.0217 - acc: 0.9927 - val_loss: 0.0179 - val_acc: 0.9935\n",
      "Epoch 7/10\n",
      "12000/12000 [==============================] - 1s 68us/step - loss: 0.0199 - acc: 0.9930 - val_loss: 0.0153 - val_acc: 0.9940\n",
      "Epoch 8/10\n",
      "12000/12000 [==============================] - 1s 66us/step - loss: 0.0189 - acc: 0.9940 - val_loss: 0.0210 - val_acc: 0.9910\n",
      "Epoch 9/10\n",
      "12000/12000 [==============================] - 1s 66us/step - loss: 0.0177 - acc: 0.9941 - val_loss: 0.0137 - val_acc: 0.9935\n",
      "Epoch 10/10\n",
      "12000/12000 [==============================] - 1s 66us/step - loss: 0.0161 - acc: 0.9948 - val_loss: 0.0192 - val_acc: 0.9930\n"
     ]
    }
   ],
   "source": [
    "model_third_first = clone_model(model)\n",
    "model_third_first.compile(loss='categorical_crossentropy' , optimizer=rmsprop(lr=0.001), metrics=['acc'])\n",
    "hist = model_third_first.fit(x_train_third_first,y_train_third_first,\n",
    "                 epochs=10,\n",
    "                 shuffle=True,\n",
    "                 batch_size=100,\n",
    "                 validation_data=(x_test_third_first,y_test_third_first))\n",
    "save_model(model_third_first,\"../Models/model_third_first\",overwrite=True)\n",
    "clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18000 samples, validate on 3000 samples\n",
      "Epoch 1/10\n",
      "18000/18000 [==============================] - 1s 75us/step - loss: 0.3733 - acc: 0.8289 - val_loss: 0.2871 - val_acc: 0.8800\n",
      "Epoch 2/10\n",
      "18000/18000 [==============================] - 1s 70us/step - loss: 0.2794 - acc: 0.8861 - val_loss: 0.2577 - val_acc: 0.8960\n",
      "Epoch 3/10\n",
      "18000/18000 [==============================] - 1s 67us/step - loss: 0.2541 - acc: 0.8948 - val_loss: 0.2383 - val_acc: 0.9067\n",
      "Epoch 4/10\n",
      "18000/18000 [==============================] - 1s 67us/step - loss: 0.2388 - acc: 0.9033 - val_loss: 0.2380 - val_acc: 0.9030\n",
      "Epoch 5/10\n",
      "18000/18000 [==============================] - 1s 67us/step - loss: 0.2280 - acc: 0.9086 - val_loss: 0.2222 - val_acc: 0.9100\n",
      "Epoch 6/10\n",
      "18000/18000 [==============================] - 1s 67us/step - loss: 0.2212 - acc: 0.9123 - val_loss: 0.2419 - val_acc: 0.8980\n",
      "Epoch 7/10\n",
      "18000/18000 [==============================] - 1s 66us/step - loss: 0.2170 - acc: 0.9126 - val_loss: 0.2117 - val_acc: 0.9143\n",
      "Epoch 8/10\n",
      "18000/18000 [==============================] - 1s 67us/step - loss: 0.2131 - acc: 0.9149 - val_loss: 0.2062 - val_acc: 0.9137\n",
      "Epoch 9/10\n",
      "18000/18000 [==============================] - 1s 67us/step - loss: 0.2079 - acc: 0.9174 - val_loss: 0.2506 - val_acc: 0.8993\n",
      "Epoch 10/10\n",
      "18000/18000 [==============================] - 1s 67us/step - loss: 0.2026 - acc: 0.9205 - val_loss: 0.1987 - val_acc: 0.9213\n"
     ]
    }
   ],
   "source": [
    "model_third_second = clone_model(model)\n",
    "model_third_second.compile(loss='categorical_crossentropy' , optimizer=rmsprop(lr=0.001), metrics=['acc'])\n",
    "hist = model_third_second.fit(x_train_third_second,y_train_third_second,\n",
    "                 epochs=10,\n",
    "                 shuffle=True,\n",
    "                 batch_size=100,\n",
    "                 validation_data=(x_test_third_second,y_test_third_second))\n",
    "save_model(model_third_second,\"../Models/model_third_second\",overwrite=True)\n",
    "clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18000 samples, validate on 3000 samples\n",
      "Epoch 1/10\n",
      "18000/18000 [==============================] - 1s 75us/step - loss: 0.1949 - acc: 0.9206 - val_loss: 0.0979 - val_acc: 0.9637\n",
      "Epoch 2/10\n",
      "18000/18000 [==============================] - 1s 73us/step - loss: 0.0960 - acc: 0.9662 - val_loss: 0.0711 - val_acc: 0.9723\n",
      "Epoch 3/10\n",
      "18000/18000 [==============================] - 1s 74us/step - loss: 0.0791 - acc: 0.9727 - val_loss: 0.0591 - val_acc: 0.9783\n",
      "Epoch 4/10\n",
      "18000/18000 [==============================] - 1s 67us/step - loss: 0.0655 - acc: 0.9781 - val_loss: 0.0525 - val_acc: 0.9793\n",
      "Epoch 5/10\n",
      "18000/18000 [==============================] - 1s 66us/step - loss: 0.0571 - acc: 0.9811 - val_loss: 0.0497 - val_acc: 0.9817\n",
      "Epoch 6/10\n",
      "18000/18000 [==============================] - 1s 67us/step - loss: 0.0493 - acc: 0.9835 - val_loss: 0.0406 - val_acc: 0.9843\n",
      "Epoch 7/10\n",
      "18000/18000 [==============================] - 1s 69us/step - loss: 0.0450 - acc: 0.9852 - val_loss: 0.0370 - val_acc: 0.9873\n",
      "Epoch 8/10\n",
      "18000/18000 [==============================] - 1s 68us/step - loss: 0.0416 - acc: 0.9869 - val_loss: 0.0346 - val_acc: 0.9873\n",
      "Epoch 9/10\n",
      "18000/18000 [==============================] - 1s 67us/step - loss: 0.0381 - acc: 0.9872 - val_loss: 0.0343 - val_acc: 0.9880\n",
      "Epoch 10/10\n",
      "18000/18000 [==============================] - 1s 68us/step - loss: 0.0372 - acc: 0.9878 - val_loss: 0.0388 - val_acc: 0.9867\n"
     ]
    }
   ],
   "source": [
    "model_third_third = clone_model(model)\n",
    "model_third_third.compile(loss='categorical_crossentropy' , optimizer=rmsprop(lr=0.001), metrics=['acc'])\n",
    "hist = model_third_third.fit(x_train_third_third,y_train_third_third,\n",
    "                 epochs=10,\n",
    "                 shuffle=True,\n",
    "                 batch_size=100,\n",
    "                 validation_data=(x_test_third_third,y_test_third_third))\n",
    "save_model(model_third_third,\"../Models/model_third_third\",overwrite=True)\n",
    "clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected activation_2 to have shape (2,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-1e5876d21ec6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m                  \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                  \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                  validation_data=(x_test_third_fourth,y_test_third_fourth))\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_third_fourth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"../Models/model_third_fourth\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mclear_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    948\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    951\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    785\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training_utils.pyc\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    135\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    138\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected activation_2 to have shape (2,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "model_third_fourth = clone_model(model)\n",
    "model_third_fourth.compile(loss='categorical_crossentropy' , optimizer=rmsprop(lr=0.001), metrics=['acc'])\n",
    "hist = model_third_fourth.fit(x_train_third_fourth,y_train_third_fourth,\n",
    "                 epochs=10,\n",
    "                 shuffle=True,\n",
    "                 batch_size=100,\n",
    "                 validation_data=(x_test_third_fourth,y_test_third_fourth))\n",
    "save_model(model_third_fourth,\"../Models/model_third_fourth\",overwrite=True)\n",
    "clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
